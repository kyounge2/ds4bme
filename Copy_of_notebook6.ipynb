{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of notebook6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyounge2/my_ds4bme/blob/master/Copy_of_notebook6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QliNPatWGeQF",
        "colab_type": "text"
      },
      "source": [
        "This example from the pytorch documentation [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) displays generating random y ad x dat and fitting a multi-layer neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54gmWH2_GILQ",
        "colab_type": "code",
        "outputId": "4376e975-3c21-4ec9-ea4a-89d5a8921799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 1.8096396923065186\n",
            "199 0.018288275226950645\n",
            "299 0.00045988932834006846\n",
            "399 1.8376693333266303e-05\n",
            "499 9.53760491029243e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWcVUq9RGxTB",
        "colab_type": "text"
      },
      "source": [
        "Let's update that example for our setting using the voxel level data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62iyChkPGwZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.linear_model as lm\n",
        "## this sets some style parameters\n",
        "sns.set()\n",
        "\n",
        "## Download in the data if it's not already there\n",
        "! if [ ! -e oasis.csv ]; \\\n",
        "  then wget https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv; \\\n",
        "fi;\n",
        "\n",
        "## Read in the data and display a few rows\n",
        "dat = pd.read_csv(\"oasis.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neIwiOlRTIor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainFraction = .75\n",
        "\n",
        "sample = np.random.uniform(size = 100) < trainFraction\n",
        "trainingDat = dat[sample]\n",
        "testingDat = dat[~sample]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tVT-AsOTPgN",
        "colab_type": "code",
        "outputId": "36546e0d-e95c-4898-e791-953a13c14fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "x = torch.from_numpy(dat[['FLAIR','T1', 'T2', 'FLAIR_10', 'T1_10', 'T2_10', 'FLAIR_20']].values)\n",
        "y = torch.from_numpy(dat[['PD']].values)\n",
        "\n",
        "##pytorch wants type as float\n",
        "x = x.float()\n",
        "y = y.float()\n",
        "\n",
        "xtraining = x[sample]\n",
        "xtesting = x[~sample]\n",
        "ytraining = y[sample]\n",
        "ytesting = y[~sample]\n",
        "\n",
        "[\n",
        " xtraining.size(),\n",
        " ytraining.size(),\n",
        " xtesting.size(),\n",
        " ytesting.size(),\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([100, 7]),\n",
              " torch.Size([100, 1]),\n",
              " torch.Size([100, 7]),\n",
              " torch.Size([100, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2UxGTdkaBdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define the model\n",
        "## Dimension of the hidden layer\n",
        "H = 10\n",
        "\n",
        "## Number of predictors\n",
        "D_in = xtraining.size()[1]\n",
        "D_out = 1\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnoPaqh7Rm2M",
        "colab_type": "code",
        "outputId": "02b3a512-e277-464b-df4d-52fa4cafec10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(500):\n",
        "    y_pred = model(xtraining)\n",
        "    loss = loss_fn(y_pred, ytraining)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 0.2803665101528168\n",
            "199 0.16991060972213745\n",
            "299 0.10219474881887436\n",
            "399 0.06100039929151535\n",
            "499 0.03614221513271332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZswhY21bcwK",
        "colab_type": "code",
        "outputId": "6b492e6a-f94a-4465-eac3-e9e129426da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "## try prediction\n",
        "ytesting_pred = model(xtesting)\n",
        "a = ytesting_pred.detach().numpy()\n",
        "\n",
        "plt.scatter(a[:,0], ytesting[:,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f16782094a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEBCAYAAACXArmGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}